{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNP8LgRTsSygg3CgPoyhKk7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranan71/AIMLtraining_Prakash_Narayanan_M/blob/main/audioSpectrogram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2CfO33G4vYZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OCLRhVQsuYOl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import wave\n",
        "import pylab\n",
        "from pathlib import Path\n",
        "from scipy import signal\n",
        "from scipy.io import wavfile\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EmtbFgHVvZ7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set paths to input and output data\n",
        "INPUT_DIR = '/kaggle/input/free-spoken-digits/free-spoken-digit-dataset-master/recordings/'\n",
        "OUTPUT_DIR = '/kaggle/working/'"
      ],
      "metadata": {
        "id": "hMwR52KEvbjR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print names of 10 WAV files from the input path\n",
        "parent_list = os.listdir(INPUT_DIR)\n",
        "for i in range(10):\n",
        "    print(parent_list[i])"
      ],
      "metadata": {
        "id": "Byx7dw6-veqN",
        "outputId": "509e146b-210d-4fbd-f213-12a301a76da2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-18a3aee5b6de>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Print names of 10 WAV files from the input path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparent_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/free-spoken-digits/free-spoken-digit-dataset-master/recordings/'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0-PeJRRpwiUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Plot first 5 WAV files as a waveform and a frequency spectrum\n",
        "for i in range(5):\n",
        "    signal_wave = wave.open(os.path.join(INPUT_DIR, parent_list[i]), 'r')\n",
        "    sample_rate = 16000\n",
        "    sig = np.frombuffer(signal_wave.readframes(sample_rate), dtype=np.int16)\n",
        "\n",
        "    plt.figure(figsize=(12,12))\n",
        "    plot_a = plt.subplot(211)\n",
        "    plot_a.set_title(parent_list[i])\n",
        "    plot_a.plot(sig)\n",
        "    plot_a.set_xlabel('sample rate * time')\n",
        "    plot_a.set_ylabel('energy')\n",
        "\n",
        "    plot_b = plt.subplot(212)\n",
        "    plot_b.specgram(sig, NFFT=1024, Fs=sample_rate, noverlap=900)\n",
        "    plot_b.set_xlabel('Time')\n",
        "    plot_b.set_ylabel('Frequency')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "2LWEvKTnwigB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Utility function to get sound and frame rate info\n",
        "def get_wav_info(wav_file):\n",
        "    wav = wave.open(wav_file, 'r')\n",
        "    frames = wav.readframes(-1)\n",
        "    sound_info = pylab.frombuffer(frames, 'int16')\n",
        "    frame_rate = wav.getframerate()\n",
        "    wav.close()\n",
        "    return sound_info, frame_rate\n",
        "\n",
        "# For every recording, make a spectogram and save it as label_speaker_no.png\n",
        "if not os.path.exists(os.path.join(OUTPUT_DIR, 'audio-images')):\n",
        "    os.mkdir(os.path.join(OUTPUT_DIR, 'audio-images'))\n",
        "\n",
        "for filename in os.listdir(INPUT_DIR):\n",
        "    if \"wav\" in filename:\n",
        "        file_path = os.path.join(INPUT_DIR, filename)\n",
        "        file_stem = Path(file_path).stem\n",
        "        target_dir = f'class_{file_stem[0]}'\n",
        "        dist_dir = os.path.join(os.path.join(OUTPUT_DIR, 'audio-images'), target_dir)\n",
        "        file_dist_path = os.path.join(dist_dir, file_stem)\n",
        "        if not os.path.exists(file_dist_path + '.png'):\n",
        "            if not os.path.exists(dist_dir):\n",
        "                os.mkdir(dist_dir)\n",
        "            file_stem = Path(file_path).stem\n",
        "            sound_info, frame_rate = get_wav_info(file_path)\n",
        "            pylab.specgram(sound_info, Fs=frame_rate)\n",
        "            pylab.savefig(f'{file_dist_path}.png')\n",
        "            pylab.close()\n",
        "\n",
        "# Print the ten classes in our dataset\n",
        "path_list = os.listdir(os.path.join(OUTPUT_DIR, 'audio-images'))\n",
        "print(\"Classes: \\n\")\n",
        "for i in range(10):\n",
        "    print(path_list[i])\n",
        "\n",
        "# File names for class 1\n",
        "path_list = os.listdir(os.path.join(OUTPUT_DIR, 'audio-images/class_1'))\n",
        "print(\"\\nA few example files: \\n\")\n",
        "for i in range(10):\n",
        "    pri\n",
        "\n",
        "nt(path_list[i])"
      ],
      "metadata": {
        "id": "XNw7WtSvwyAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vrLmel9NwywQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare constants\n",
        "IMAGE_HEIGHT = 256\n",
        "IMAGE_WIDTH = 256\n",
        "BATCH_SIZE = 32\n",
        "N_CHANNELS = 3\n",
        "N_CLASSES = 10\n",
        "\n",
        "# Make a dataset containing the training spectrograms\n",
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             validation_split=0.2,\n",
        "                                             directory=os.path.join(OUTPUT_DIR, 'audio-images'),\n",
        "                                             shuffle=True,\n",
        "                                             color_mode='rgb',\n",
        "                                             image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
        "                                             subset=\"training\",\n",
        "                                             seed=0)\n",
        "\n",
        "# Make a dataset containing the validation spectrogram\n",
        "valid_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             validation_split=0.2,\n",
        "                                             directory=os.path.join(OUTPUT_DIR, 'audio-images'),\n",
        "                                             shuffle=True,\n",
        "                                             color_mode='rgb',\n",
        "                                             image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
        "                                             subset=\"validation\",\n",
        "                                             seed=0)\n",
        "\n",
        "Found 3000 files belonging to 10 classes.\n",
        "Using 2400 files for training.\n",
        "Found 3000 files belonging to 10 classes.\n",
        "Using 600 files for validation.\n",
        "\n",
        "Let's quickly visualize a few of the generated spectograms and their labels.\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "for images, labels in train_dataset.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        p\n",
        "\n",
        "lt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(int(labels[i]))\n",
        "        plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "SxWrrx_2w6Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to prepare our datasets for modelling\n",
        "def prepare(ds, augment=False):\n",
        "    # Define our one transformation\n",
        "    rescale = tf.keras.Sequential([tf.keras.layers.experimental.preprocessing.Rescaling(1./255)])\n",
        "    flip_and_rotate = tf.keras.Sequential([\n",
        "        tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
        "        tf.keras.layers.experimental.preprocessing.RandomRotation(0.2)\n",
        "    ])\n",
        "\n",
        "    # Apply rescale to both datasets and augmentation only to training\n",
        "    ds = ds.map(lambda x, y: (rescale(x, training=True), y))\n",
        "    if augment: ds = ds.map(lambda x, y: (flip_and_rotate(x, training=True), y))\n",
        "    return ds\n",
        "\n",
        "train_dataset = prepare(train_dataset, augment=False)\n",
        "valid_dataset = prepare(valid_dataset, augment=False)"
      ],
      "metadata": {
        "id": "vNov6IuVw-m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create CNN model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, N_CHANNELS)))\n",
        "model.add(tf.keras.layers.Conv2D(32, 3, strides=2, padding='same', activation='relu'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(N_CLASSES, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=tf.keras.optimizers.RMSprop(),\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "# Train model for 10 epochs, capture the history\n",
        "history = model.fit(train_dataset, epochs=10, validation_data=valid_dataset)"
      ],
      "metadata": {
        "id": "hr1tuaxVxJY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Plot the loss curves for training and validation.\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "epochs = range(1, len(loss_values)+1)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "IhhMa0duxPhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the accuracy curves for training and validation.\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "epochs = range(1, len(acc_values)+1)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(epochs, acc_values, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc_values, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qUWvTfh2xTHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the final loss and accuracy\n",
        "final_loss, final_acc = model.evaluate(valid_dataset, verbose=0)\n",
        "print(\"Final loss: {0:.6f}, final accuracy: {1:.6f}\".format(final_loss, final_acc))"
      ],
      "metadata": {
        "id": "KVV3LUk5xV_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Clean the output dir\n",
        "import shutil\n",
        "shutil.rmtree('/kaggle/working/audio-images')\n",
        "\n"
      ],
      "metadata": {
        "id": "aI2Pgg22xYHD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}